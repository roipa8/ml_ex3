{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/ass3.pickle', 'rb') as handle:\n",
    "    data = pd.read_pickle(handle)\n",
    "\n",
    "X_train, y_train = data['train']\n",
    "X_dev, y_dev = data['dev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Train data:')\n",
    "rows_train, columns_train = X_train.shape\n",
    "print(f'Number of Rows: {rows_train}, Number of Coloumns: {columns_train}')\n",
    "\n",
    "print('Dev data:')\n",
    "rows_dev, columns_dev = X_dev.shape\n",
    "print(f'Number of Rows: {rows_dev}, Number of Coloumns: {columns_dev}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we run LazyPredict to choose the few best algorithms to continue with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazypredict.Supervised import LazyRegressor\n",
    "\n",
    "reg = LazyRegressor(verbose=0,ignore_warnings=True, custom_metric=None)\n",
    "models,predictions = reg.fit(X_train, X_dev, y_train, y_dev)\n",
    "\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data has 13 features, we might need to reduce some of them based on their importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "\n",
    "def select_k_best(k, X, y):\n",
    "    k_best = SelectKBest(mutual_info_regression, k=k)\n",
    "    k_best.fit(X, y)\n",
    "    return (k_best.transform(X), k_best.transform(X_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "def cross_validation(reg, X, y, k):\n",
    "    kf = KFold(n_splits=5,shuffle=True)\n",
    "    avg = 0\n",
    "    count = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        reg.fit(X_train,y_train)\n",
    "        avg += mean_absolute_error(y_test, reg.predict(X_test))\n",
    "        count+=1\n",
    "    return avg / count\n",
    "\n",
    "def fit_predict(regressor, X_train, y_train, X_dev):\n",
    "    sc = StandardScaler()\n",
    "    scaled_X_train = sc.fit_transform(X_train)\n",
    "    regressor.fit(scaled_X_train, y_train)\n",
    "\n",
    "    train_score = mean_absolute_error(regressor.predict(scaled_X_train), y_train)\n",
    "    dev_score = mean_absolute_error(regressor.predict(sc.transform(X_dev)), y_dev)\n",
    "    over_fitting = train_score - dev_score\n",
    "\n",
    "    return f'train score: {round(train_score, 3)} test score {round(dev_score, 3)} overfit {round(over_fitting, 3)}'\n",
    "\n",
    "def fit_predict_poly(regressor, X_train, y_train, X_dev, y_dev):\n",
    "    transformed_train = PolynomialFeatures(2).fit_transform(X_train)\n",
    "    transformed_dev = PolynomialFeatures(2).fit_transform(X_dev)\n",
    "    sc = StandardScaler()\n",
    "    scaled_X_train = sc.fit_transform(transformed_train)\n",
    "\n",
    "    regressor.fit(scaled_X_train, y_train)\n",
    "\n",
    "    train_score = mean_absolute_error(regressor.predict(scaled_X_train), y_train)\n",
    "    dev_score = mean_absolute_error(regressor.predict(sc.transform(transformed_dev)), y_dev)\n",
    "    over_fitting = train_score - dev_score\n",
    "\n",
    "    return f'train score: {round(train_score, 3)} test score {round(dev_score, 3)} overfit {round(over_fitting, 3)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial test of the best algoriths using cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "def check_models(X, y, k):\n",
    "    for reg in regs:\n",
    "        print(reg)\n",
    "        for i in range(1,14):\n",
    "            X_train, X_dev = select_k_best(i, X, y)\n",
    "        print(f'cv {cross_validation(regs[reg], np.array(X_train), np.array(y), k)} Predict {fit_predict(regs[reg], X_train, y, X_dev)} Polynomial {fit_predict_poly(regs[reg], X_train, y, X_dev, y_dev)}')\n",
    "\n",
    "regs = {\n",
    "    'Random Forest':RandomForestRegressor(random_state=42),\n",
    "    'AdaBoost':AdaBoostRegressor(random_state=42),\n",
    "    'Histogram Gradient Boosting':HistGradientBoostingRegressor(random_state = 42),\n",
    "    'XGBoost':XGBRegressor(random_state = 42),\n",
    "    'Light GBM':LGBMRegressor(random_state = 42),\n",
    "    'ExtraTreesRegressor' : ExtraTreesRegressor(random_state=42),\n",
    "    'Bagging Regressor':BaggingRegressor(random_state=42)\n",
    "    }\n",
    "\n",
    "check_models(X_train, y_train, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best hyperparameters for each model using RandomizedSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#The function to measure the quality of a split\n",
    "criterion = ['squared_error', 'absolute_error']\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['log2', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "rf_grid =  {'criterion' : criterion,\n",
    "            'n_estimators': n_estimators,\n",
    "            'max_features': max_features,\n",
    "            'max_depth': max_depth,\n",
    "            'min_samples_split': min_samples_split,\n",
    "            'min_samples_leaf': min_samples_leaf,\n",
    "            'bootstrap': bootstrap}\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = RandomForestRegressor(random_state=42), param_distributions = rf_grid)\n",
    "rf_random.fit(X_train, y_train)\n",
    "print(rf_random.best_params_)\n",
    "print(rf_random.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid = {\n",
    "    \"learning_rate\"     : [0.05, 0.1, 0.15, 0.2, 0.25, 0.3] ,\n",
    "    \"max_depth\"         : [3, 4, 5, 6, 8, 10, 12, 15],\n",
    "    \"min_child_weight\"  : [1, 3, 5, 7],\n",
    "    \"gamma\"             : [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "    \"colsample_bytree\"  : [0.3, 0.4, 0.5, 0.7]\n",
    "}\n",
    "\n",
    "xgb_random = RandomizedSearchCV(XGBRegressor(random_state=42), param_distributions=xgb_grid, n_iter=50, scoring='neg_mean_absolute_error', n_jobs=-1, cv=5, verbose=3)\n",
    "xgb_random.fit(X_train, y_train)\n",
    "print(xgb_random.best_params_)\n",
    "print(xgb_random.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgr_grid = {\n",
    "    'n_estimators': [20,50,100],\n",
    "    'max_samples': [0.5,0.1],\n",
    "    'max_features': [1,2,4,6,8],\n",
    "    'bootstrap': [True, False],\n",
    "}\n",
    "\n",
    "bgr_random = RandomizedSearchCV(BaggingRegressor(random_state=42), param_distributions=bgr_grid, n_iter=50, scoring='neg_mean_absolute_error', n_jobs=-1, cv=5, verbose=3)\n",
    "\n",
    "bgr_random.fit(X_train, y_train)\n",
    "print(bgr_random.best_params_)\n",
    "print(bgr_random.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr_grid = {\n",
    "    'n_estimators': [10,50,100],\n",
    "    'max_depth': [2,8,16,32,50],\n",
    "    'min_samples_split': [2,4,6],\n",
    "    'min_samples_leaf': [1,2],\n",
    "    'max_features': ['auto','sqrt','log2'],    \n",
    "    'bootstrap': [True, False],\n",
    "    'warm_start': [True, False],\n",
    "}\n",
    "\n",
    "xtr_random = RandomizedSearchCV(ExtraTreesRegressor(random_state=42), param_distributions=xtr_grid, n_iter=50, scoring='neg_mean_absolute_error', n_jobs=-1, cv=5, verbose=3)\n",
    "\n",
    "xtr_random.fit(X_train, y_train)\n",
    "print(xtr_random.best_params_)\n",
    "print(xtr_random.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the new models with the 'dev' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "        random_state = 42, bootstrap=True, max_depth=70,\n",
    "        min_samples_leaf=4, min_samples_split=10, n_estimators=400)\n",
    "\n",
    "xgb = XGBRegressor(\n",
    "        base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "        colsample_bynode=1, colsample_bytree=0.7, gamma=0.0, gpu_id=-1,\n",
    "        importance_type='gain', interaction_constraints='',\n",
    "        learning_rate=0.1, max_delta_step=0, max_depth=12,\n",
    "        min_child_weight=7, monotone_constraints='()',\n",
    "        n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=42,\n",
    "        reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
    "        tree_method='exact', validate_parameters=1, verbosity=None)\n",
    "\n",
    "xtr = ExtraTreesRegressor(random_state = 42)\n",
    "\n",
    "bg = BaggingRegressor(random_state = 42)\n",
    "\n",
    "clfs = {'xgb': xgb, 'xtr': xtr, 'bg': bg, 'rf': rf}\n",
    "\n",
    "for clf in clfs:\n",
    "        print(f'{clf}: {fit_predict(clfs[clf], X_train, y_train, X_dev)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b671c20432fcd147198c92e7f072af9e705f087eb990bee22b07f08caab9f630"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
